#%Module -*- tcl -*-
##

lappend auto_path /shared/ucl/apps/modulelibs/UsefulModuleFunctions
package require modulefunctions 1.0

proc ModulesHelp { } {

  puts stderr {[Intel MPI/2019.5.281] This is Intel's MPI implementation, version 2019.5.281, which is bundled with compiler package version 2019.Update5.}

}

module-whatis {[Intel MPI/2019.5.281] This is Intel's MPI implementation, version 2019.5.281, which is bundled with compiler package version 2019.Update5.}


conflict intel

set              prefix               /no/prefix/given

# Add compatibility library for MPI built for infiniband not omnipath
if { [modulefunctions::isCluster grace] } {
    setenv FI_VERBS_IFACE ib0:1
}

setenv GERUN_LAUNCHER intel
setenv MPI_HOME /shared/ucl/apps/intel/2019.Update5/impi/2019.5.281
setenv I_MPI_ROOT /shared/ucl/apps/intel/2019.Update5/impi/2019.5.281
setenv I_MPI_CC icc
setenv I_MPI_CXX icpc
setenv I_MPI_F90 ifort
setenv I_MPI_F77 ifort
setenv FI_PROVIDER_PATH /shared/ucl/apps/intel/2019.Update5/impi/2019.5.281/intel64/libfabric/lib/prov
prepend-path PATH /shared/ucl/apps/intel/2019.Update5/impi/2019.5.281/intel64/bin
prepend-path MANPATH /shared/ucl/apps/intel/2019.Update5/impi/2019.5.281/man
prepend-path LD_LIBRARY_PATH /shared/ucl/apps/intel/2019.Update5/impi/2019.5.281/intel64/lib
prepend-path LIBRARY_PATH /shared/ucl/apps/intel/2019.Update5/impi/2019.5.281/intel64/lib
prepend-path CPATH /shared/ucl/apps/intel/2019.Update5/impi/2019.5.281/intel64/include
prepend-path CMAKE_PREFIX_PATH /shared/ucl/apps/intel/2019.Update5/impi/2019.5.281
prepend-path PATH /shared/ucl/apps/intel/2019.Update5/impi/2019.5.281/intel64/libfabric/bin
prepend-path LD_LIBRARY_PATH /shared/ucl/apps/intel/2019.Update5/impi/2019.5.281/intel64/libfabric/lib
prepend-path LIBRARY_PATH /shared/ucl/apps/intel/2019.Update5/impi/2019.5.281/intel64/libfabric/lib

# Check to see whether this is running in a scheduled environment (NHOSTS>1) and
# if not, set shared memory only.
if { [info exists ::env(NHOSTS) ] } {

        # Scheduled.
        # Check to see whether we have more than one host and if so set ofi and shared
        # memory, otherwise, set shared memory.
        if { $::env(NHOSTS) > 1} {

                # Multi-node, scheduled.
                setenv I_MPI_FABRICS shm:ofi
        } else {

                # Single node, scheduled.
                setenv I_MPI_FABRICS shm
        }
} else {

        # Not scheduled.
        setenv I_MPI_FABRICS shm
}

# 2019 Intel MPI segfaults on certain cpusets if hwloc is used
# so switch hwloc to ipl if the cpuset is restricted
if {[modulefunctions::isCpusetLimited]} {
    setenv I_MPI_HYDRA_TOPOLIB ipl
} else {
    # the default
    setenv I_MPI_HYDRA_TOPOLIB hwloc
}

