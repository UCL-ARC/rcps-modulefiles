#%Module -*- tcl -*-
##
## generated by ccspapp on 2024-08-22 11:45:24 +0100
## using cmd line:
##  "-d -c mpi -o /shared/ucl/apps/intel/2024.0.1/.uclrc_modules/mpi/mpi/intel/2021.11/intel -v GERUN_LAUNCHER=intel -v MPI_HOME=/shared/ucl/apps/intel/2024.0.1/mpi/2021.11 -v I_MPI_ROOT=/shared/ucl/apps/intel/2024.0.1/mpi/2021.11 -v I_MPI_CC=icc -v I_MPI_CXX=icpc -v I_MPI_F90=ifort -v I_MPI_F77=ifort -v CCL_ROOT=/shared/ucl/apps/intel/2024.0.1/ccl/2021.11 -v CCL_CONFIGURATION=cpu_gpu_dpcpp -e CLASSPATH:/shared/ucl/apps/intel/2024.0.1/mpi/2021.11/share/java/mpi.jar -e PATH:/shared/ucl/apps/intel/2024.0.1/mpi/2021.11/bin -e LD_LIBRARY_PATH:/shared/ucl/apps/intel/2024.0.1/mpi/2021.11/lib -e LD_LIBRARY_PATH:/shared/ucl/apps/intel/2024.0.1/ccl/2021.11/lib -e LIBRARY_PATH:/shared/ucl/apps/intel/2024.0.1/mpi/2021.11/lib -e LIBRARY_PATH:/shared/ucl/apps/intel/2024.0.1/ccl/2021.11/lib -e CPATH:/shared/ucl/apps/intel/2024.0.1/mpi/2021.11/include -e CPATH:/shared/ucl/apps/intel/2024.0.1/ccl/2021.11/include -e MANPATH:/shared/ucl/apps/intel/2024.0.1/mpi/2021.11/share/man -e CMAKE_PREFIX_PATH:/shared/ucl/apps/intel/2024.0.1/mpi/2021.11 -e PKG_CONFIG_PATH:/shared/ucl/apps/intel/2024.0.1/mpi/2021.11/lib/pkgconfig -e PKG_CONFIG_PATH:/shared/ucl/apps/intel/2024.0.1/ccl/2021.11/lib/pkgconfig -v FI_PROVIDER_PATH=/shared/ucl/apps/intel/2024.0.1/mpi/2021.11/libfabric/lib/prov:/usr/lib64/libfabric -e PATH:/shared/ucl/apps/intel/2024.0.1/mpi/2021.11/libfabric/bin -e LD_LIBRARY_PATH:/shared/ucl/apps/intel/2024.0.1/mpi/2021.11/libfabric/lib -e LIBRARY_PATH:/shared/ucl/apps/intel/2024.0.1/mpi/2021.11/libfabric/lib -w [Intel MPI/2021.11] This is Intel's MPI implementation, version 2021.11, which is bundled with compiler package version 2024.0.1."

proc ModulesHelp { } {

  puts stderr {[Intel MPI/2021.11] This is Intel's MPI implementation, version 2021.11, which is bundled with compiler package version 2024.0.1.}

}

module-whatis {[Intel MPI/2021.11] This is Intel's MPI implementation, version 2021.11, which is bundled with compiler package version 2024.0.1.}


conflict mpi

set              prefix               /no/prefix/given

setenv GERUN_LAUNCHER intel
setenv MPI_HOME /shared/ucl/apps/intel/2024.0.1/mpi/2021.11
setenv I_MPI_ROOT /shared/ucl/apps/intel/2024.0.1/mpi/2021.11
setenv I_MPI_CC icc
setenv I_MPI_CXX icpc
setenv I_MPI_F90 ifort
setenv I_MPI_F77 ifort
setenv CCL_ROOT /shared/ucl/apps/intel/2024.0.1/ccl/2021.11
setenv CCL_CONFIGURATION cpu_gpu_dpcpp
setenv FI_PROVIDER_PATH /shared/ucl/apps/intel/2024.0.1/mpi/2021.11/libfabric/lib/prov:/usr/lib64/libfabric
prepend-path CLASSPATH /shared/ucl/apps/intel/2024.0.1/mpi/2021.11/share/java/mpi.jar
prepend-path PATH /shared/ucl/apps/intel/2024.0.1/mpi/2021.11/bin
prepend-path LD_LIBRARY_PATH /shared/ucl/apps/intel/2024.0.1/mpi/2021.11/lib
prepend-path LD_LIBRARY_PATH /shared/ucl/apps/intel/2024.0.1/ccl/2021.11/lib
prepend-path LIBRARY_PATH /shared/ucl/apps/intel/2024.0.1/mpi/2021.11/lib
prepend-path LIBRARY_PATH /shared/ucl/apps/intel/2024.0.1/ccl/2021.11/lib
prepend-path CPATH /shared/ucl/apps/intel/2024.0.1/mpi/2021.11/include
prepend-path CPATH /shared/ucl/apps/intel/2024.0.1/ccl/2021.11/include
prepend-path MANPATH /shared/ucl/apps/intel/2024.0.1/mpi/2021.11/share/man
prepend-path CMAKE_PREFIX_PATH /shared/ucl/apps/intel/2024.0.1/mpi/2021.11
prepend-path PKG_CONFIG_PATH /shared/ucl/apps/intel/2024.0.1/mpi/2021.11/lib/pkgconfig
prepend-path PKG_CONFIG_PATH /shared/ucl/apps/intel/2024.0.1/ccl/2021.11/lib/pkgconfig
prepend-path PATH /shared/ucl/apps/intel/2024.0.1/mpi/2021.11/libfabric/bin
prepend-path LD_LIBRARY_PATH /shared/ucl/apps/intel/2024.0.1/mpi/2021.11/libfabric/lib
prepend-path LIBRARY_PATH /shared/ucl/apps/intel/2024.0.1/mpi/2021.11/libfabric/lib

# Check to see whether this is running in a scheduled environment (NHOSTS>1) and
# if not, set shared memory only.
if { [info exists ::env(NHOSTS) ] } {

        # Scheduled.
        # Check to see whether we have more than one host and if so set ofi and shared
        # memory, otherwise, set shared memory.
        if { $::env(NHOSTS) > 1} {

                # Multi-node, scheduled.
                setenv I_MPI_FABRICS shm:ofi
        } else {

                # Single node, scheduled.
                setenv I_MPI_FABRICS shm
        }
} else {

        # Not scheduled.
        setenv I_MPI_FABRICS shm
}

