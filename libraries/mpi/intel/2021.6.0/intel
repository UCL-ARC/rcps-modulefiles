#%Module -*- tcl -*-
##
## generated by ccspapp on 2022-09-05 11:31:00 +0100
## using cmd line:
##  "-d -c mpi/intel -o /shared/ucl/apps/intel/2022.2/.uclrc_modules/mpi/mpi/intel/2021.6.0/intel -v GERUN_LAUNCHER=intel -v MPI_HOME=/shared/ucl/apps/intel/2022.2/mpi/2021.6.0 -v I_MPI_ROOT=/shared/ucl/apps/intel/2022.2/mpi/2021.6.0 -v I_MPI_CC=icc -v I_MPI_CXX=icpc -v I_MPI_F90=ifort -v I_MPI_F77=ifort -v CCL_ROOT=/shared/ucl/apps/intel/2022.2/ccl/2021.6.0 -e CLASSPATH:/shared/ucl/apps/intel/2022.2/mpi/2021.6.0/lib/mpi.jar -e PATH:/shared/ucl/apps/intel/2022.2/mpi/2021.6.0/bin -e LD_LIBRARY_PATH:/shared/ucl/apps/intel/2022.2/mpi/2021.6.0/lib -e LIBRARY_PATH:/shared/ucl/apps/intel/2022.2/mpi/2021.6.0/lib -e CPATH:/shared/ucl/apps/intel/2022.2/mpi/2021.6.0/include -e MANPATH:/shared/ucl/apps/intel/2022.2/mpi/2021.6.0/man -e CMAKE_PREFIX_PATH:/shared/ucl/apps/intel/2022.2/mpi/2021.6.0 -v FI_PROVIDER_PATH=/shared/ucl/apps/intel/2022.2/mpi/2021.6.0/libfabric/lib/prov:/usr/lib64/libfabric -e PATH:/shared/ucl/apps/intel/2022.2/mpi/2021.6.0/libfabric/bin -e LD_LIBRARY_PATH:/shared/ucl/apps/intel/2022.2/mpi/2021.6.0/libfabric/lib -e LIBRARY_PATH:/shared/ucl/apps/intel/2022.2/mpi/2021.6.0/libfabric/lib -w [Intel MPI/2021.6.0] This is Intel's MPI implementation, version 2021.6.0, which is bundled with compiler package version 2022.2."

proc ModulesHelp { } {

  puts stderr {[Intel MPI/2021.6.0] This is Intel's MPI implementation, version 2021.6.0, which is bundled with compiler package version 2022.2.}

}

module-whatis {[Intel MPI/2021.6.0] This is Intel's MPI implementation, version 2021.6.0, which is bundled with compiler package version 2022.2.}


conflict mpi/intel

set              prefix               /no/prefix/given

setenv GERUN_LAUNCHER intel
setenv MPI_HOME /shared/ucl/apps/intel/2022.2/mpi/2021.6.0
setenv I_MPI_ROOT /shared/ucl/apps/intel/2022.2/mpi/2021.6.0
setenv I_MPI_CC icc
setenv I_MPI_CXX icpc
setenv I_MPI_F90 ifort
setenv I_MPI_F77 ifort
setenv CCL_ROOT /shared/ucl/apps/intel/2022.2/ccl/2021.6.0
setenv FI_PROVIDER_PATH /shared/ucl/apps/intel/2022.2/mpi/2021.6.0/libfabric/lib/prov:/usr/lib64/libfabric
prepend-path CLASSPATH /shared/ucl/apps/intel/2022.2/mpi/2021.6.0/lib/mpi.jar
prepend-path PATH /shared/ucl/apps/intel/2022.2/mpi/2021.6.0/bin
prepend-path LD_LIBRARY_PATH /shared/ucl/apps/intel/2022.2/mpi/2021.6.0/lib
prepend-path LIBRARY_PATH /shared/ucl/apps/intel/2022.2/mpi/2021.6.0/lib
prepend-path CPATH /shared/ucl/apps/intel/2022.2/mpi/2021.6.0/include
prepend-path MANPATH /shared/ucl/apps/intel/2022.2/mpi/2021.6.0/man
prepend-path CMAKE_PREFIX_PATH /shared/ucl/apps/intel/2022.2/mpi/2021.6.0
prepend-path PATH /shared/ucl/apps/intel/2022.2/mpi/2021.6.0/libfabric/bin
prepend-path LD_LIBRARY_PATH /shared/ucl/apps/intel/2022.2/mpi/2021.6.0/libfabric/lib
prepend-path LIBRARY_PATH /shared/ucl/apps/intel/2022.2/mpi/2021.6.0/libfabric/lib

# Check to see whether this is running in a scheduled environment (NHOSTS>1) and
# if not, set shared memory only.
if { [info exists ::env(NHOSTS) ] } {

        # Scheduled.
        # Check to see whether we have more than one host and if so set ofi and shared
        # memory, otherwise, set shared memory.
        if { $::env(NHOSTS) > 1} {

                # Multi-node, scheduled.
                setenv I_MPI_FABRICS shm:ofi
        } else {

                # Single node, scheduled.
                setenv I_MPI_FABRICS shm
        }
} else {

        # Not scheduled.
        setenv I_MPI_FABRICS shm
}

